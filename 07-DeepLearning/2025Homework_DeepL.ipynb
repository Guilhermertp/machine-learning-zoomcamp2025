{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Github Pytorch material!!!\n",
        "\n",
        "https://github.com/DataTalksClub/machine-learning-zoomcamp/tree/master/08-deep-learning/pytorch"
      ],
      "metadata": {
        "id": "1qCgiNaySDfE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dataset**\n",
        "\n",
        "In this homework, we'll build a model for classifying various hair types. For this, we will use the Hair Type dataset that was obtained from Kaggle and slightly rebuilt.\n",
        "\n",
        "You can download the target dataset for this homework from here:\n",
        "\n",
        "https://github.com/SVizor42/ML_Zoomcamp/releases/download/straight-curly-data/data.zip\n",
        "\n",
        "```\n",
        "!wget 'https://github.com/SVizor42/ML_Zoomcamp/releases/download/straight-curly-data/data.zip'\n",
        "unzip data.zip\n",
        "```\n",
        "\n",
        "\n",
        "In the lectures we saw how to use a pre-trained neural network. In the homework, we'll train a much smaller model from scratch.\n",
        "\n",
        "We will use PyTorch for that.\n",
        "\n",
        "You can use Google Colab or your own computer for that."
      ],
      "metadata": {
        "id": "kV6wrPOO3UJq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Preparation**\n",
        "\n",
        "The dataset contains around 1000 images of hairs in the separate folders for training and test sets.\n",
        "\n",
        "**Reproducibility**\n",
        "\n",
        "Reproducibility in deep learning is a multifaceted challenge that requires attention to both software and hardware details. In some cases, we can't guarantee exactly the same results during the same experiment runs. Therefore, in this homework we suggest to:\n",
        "\n",
        "* set the seed generators by:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "```\n",
        "\n",
        "Also, use PyTorch of version 2.8.0 (that's the one in Colab)."
      ],
      "metadata": {
        "id": "rgWSgr_z6nhW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model**\n",
        "\n",
        "For this homework we will use Convolutional Neural Network (CNN). We'll use PyTorch.\n",
        "\n",
        "You need to develop the model with following structure:\n",
        "\n",
        "* The shape for input should be `(3, 200, 200)` (channels first format in PyTorch)\n",
        "- Next, create a convolutional layer `(nn.Conv2d)`:\n",
        "  - Use 32 filters (output channels)\n",
        "  - Kernel size should be `(3, 3)` (that's the size of the filter)\n",
        "  - Use `'relu'` as activation\n",
        "- Reduce the size of the feature map with max pooling `(nn.MaxPool2d)`\n",
        "  - Set the pooling size to `(2, 2)`\n",
        "\n",
        "- Reduce the size of the feature map with max pooling `(nn.MaxPool2d)`\n",
        "  - Use 32 filters\n",
        "  - Kernel size should be `(3, 3)` (that's the size of the filter)\n",
        "\n",
        "  - Use 'relu' as activation\n",
        "\n",
        "- Reduce the size of the feature map with max pooling `(MaxPooling2D)`\n",
        "  - Set the pooling size to `(2, 2)`\n",
        "\n",
        "- Turn the multi-dimensional result into vectors using flatten or view\n",
        "- Next, add a nn.Linear layer with 64 neurons and `'relu'` activation\n",
        "- Finally, create the nn.Linear layer with 1 neuron - this will be the output\n",
        "  - The output layer should have an activation - use the appropriate activation for the binary classification case\n",
        "\n",
        "  As optimizer use `torch.optim.SGD` with the following parameters:\n",
        "\n",
        "  - `torch.optim.SGD(model.parameters(), lr=0.002, momentum=0.8)`\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1o1Z00sZ7CYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting the dataset\n",
        "!wget 'https://github.com/SVizor42/ML_Zoomcamp/releases/download/straight-curly-data/data.zip'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4mNM-TPgs1y",
        "outputId": "898f5d46-f53e-42dc-db21-5d25f6714f98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-27 12:04:40--  https://github.com/SVizor42/ML_Zoomcamp/releases/download/straight-curly-data/data.zip\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/405934815/e712cf72-f851-44e0-9c05-e711624af985?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-11-27T12%3A51%3A07Z&rscd=attachment%3B+filename%3Ddata.zip&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-11-27T11%3A50%3A40Z&ske=2025-11-27T12%3A51%3A07Z&sks=b&skv=2018-11-09&sig=vEaKFFcav%2BKKMpKJ9CtZtO80DUzZtkGM2qTrDvk27Xg%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NDI0Njg4MCwibmJmIjoxNzY0MjQ1MDgwLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.tXap18RqrGtvBJTsQFnySI787m3RdmjWWT9PEa5KB78&response-content-disposition=attachment%3B%20filename%3Ddata.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-11-27 12:04:40--  https://release-assets.githubusercontent.com/github-production-release-asset/405934815/e712cf72-f851-44e0-9c05-e711624af985?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-11-27T12%3A51%3A07Z&rscd=attachment%3B+filename%3Ddata.zip&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-11-27T11%3A50%3A40Z&ske=2025-11-27T12%3A51%3A07Z&sks=b&skv=2018-11-09&sig=vEaKFFcav%2BKKMpKJ9CtZtO80DUzZtkGM2qTrDvk27Xg%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NDI0Njg4MCwibmJmIjoxNzY0MjQ1MDgwLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.tXap18RqrGtvBJTsQFnySI787m3RdmjWWT9PEa5KB78&response-content-disposition=attachment%3B%20filename%3Ddata.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 102516572 (98M) [application/octet-stream]\n",
            "Saving to: ‘data.zip.1’\n",
            "\n",
            "data.zip.1          100%[===================>]  97.77M   254MB/s    in 0.4s    \n",
            "\n",
            "2025-11-27 12:04:40 (254 MB/s) - ‘data.zip.1’ saved [102516572/102516572]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip 'data.zip'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWkY9f7yg9SW",
        "outputId": "4211ff5b-0555-4a83-d0e4-9fd222dd94c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  data.zip\n",
            "replace data/test/curly/03312ac556a7d003f7570657f80392c34.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "_nZEeajeiWNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model\n",
        "\n",
        "For this homework we will use a **Convolutional Neural Network (CNN)** implemented in **PyTorch**.\n",
        "\n",
        "You need to develop a model with the following structure:\n",
        "\n",
        "1. **Input**\n",
        "   - Shape: `(3, 200, 200)` (channels-first format in PyTorch)\n",
        "\n",
        "2. **Convolutional Layer (`nn.Conv2d`)**\n",
        "   - Filters (output channels): **32**\n",
        "   - Kernel size: **(3, 3)**\n",
        "   - Activation: **ReLU**\n",
        "\n",
        "3. **Max Pooling Layer (`nn.MaxPool2d`)**\n",
        "   - Pool size: **(2, 2)**\n",
        "\n",
        "4. **Flatten**\n",
        "   - Convert multi-dimensional output to vectors using `.view()` or `nn.Flatten`\n",
        "\n",
        "5. **Fully Connected Layer (`nn.Linear`)**\n",
        "   - Units: **64**\n",
        "   - Activation: **ReLU**\n",
        "\n",
        "6. **Output Layer (`nn.Linear`)**\n",
        "   - Units: **1**\n",
        "   - Activation: appropriate for binary classification (e.g., **Sigmoid**)\n",
        "\n",
        "---\n",
        "\n",
        "### Optimizer\n",
        "\n",
        "Use Stochastic Gradient Descent:\n",
        "\n",
        "```python\n",
        "torch.optim.SGD(model.parameters(), lr=0.002, momentum=0.8)\n"
      ],
      "metadata": {
        "id": "oQN_7a8j9XpS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1**\n",
        "\n",
        "Which loss function you will use?\n",
        "\n",
        "* `nn.MSELoss()`\n",
        "* `nn.BCEWithLogitsLoss()`\n",
        "* `nn.CrossEntropyLoss()`\n",
        "* `nn.CosineEmbeddingLoss()`\n",
        "\n",
        "(Multiple answered can be correct, so pick any)"
      ],
      "metadata": {
        "id": "0Q2UqxCV3T_i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **Loss Function**              | **Description**                                                      | **Target Format**              | **Type of Problem**                                         | **Examples of Application**                                                                                                                                                             |\n",
        "| ------------------------------ | -------------------------------------------------------------------- | ------------------------------ | ----------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| **`nn.MSELoss()`**             | Measures squared difference between predicted and actual values      | Continuous values (float)      | **Regression**                                              | - Predicting age from an image<br>- Predicting house price<br>- Predicting coordinates<br>- Autoencoder reconstruction                                                                  |\n",
        "| **`nn.BCEWithLogitsLoss()`**   | Sigmoid + binary cross entropy (stable)                              | 0/1 or multi-label vector      | **Binary classification** or **Multi-label classification** | - “Does the image contain hair?” (yes/no)<br>- Detecting multiple diseases in X-rays<br>- Multi-label emotion detection (happy/sad/angry simultaneously)<br>- Tagging objects in images |\n",
        "| **`nn.CrossEntropyLoss()`**    | Softmax + cross-entropy for exclusive classes                        | Integer class index            | **Multi-class classification**                              | - Hair type classification (straight/wavy/curly/coily)<br>- Dog breed classification<br>- Sentiment analysis (positive/neutral/negative)<br>- Handwritten digit recognition             |\n",
        "| **`nn.CosineEmbeddingLoss()`** | Measures similarity/dissimilarity of vectors using cosine similarity | 1 (similar) or −1 (dissimilar) | **Similarity / Metric learning / Siamese networks**         | - Face verification (same person?)<br>- Signature verification<br>- Image similarity ranking<br>- Sentence embedding similarity                                                         |\n"
      ],
      "metadata": {
        "id": "XU3NYbriLHT4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "nn.BCEWithLogitsLoss()\n",
        "\n",
        "Because the model has 1 output neuron and we are performing binary classification.\n",
        "\n",
        "This is the correct and standard choice. We would use  nn.CrossEntropyLoss() when:\n",
        "✔️ We have multi-class classification\n",
        "\n",
        "(e.g., 3 classes, 10 classes, 100 classes…)\n",
        "\n",
        "Examples:\n",
        "\n",
        "MNIST → 10 digits\n",
        "\n",
        "CIFAR-10 → 10 image categories\n",
        "\n",
        "Sentiment classification → 3 labels (positive/neutral/negative)"
      ],
      "metadata": {
        "id": "np_WSa44_LbY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correct answer is **nn.BCEWithLogitsLoss()**"
      ],
      "metadata": {
        "id": "XqskvPSN_tT0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2**\n",
        "\n",
        "What's the total number of parameters of the model? You can use torchsummary or count manually.\n",
        "\n",
        "In PyTorch, you can find the total number of parameters using:\n",
        "```\n",
        "# Option 1: Using torchsummary (install with: pip install torchsummary)\n",
        "from torchsummary import summary\n",
        "summary(model, input_size=(3, 200, 200))\n",
        "\n",
        "# Option 2: Manual counting\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total parameters: {total_params}\")\n",
        "```\n",
        "* 896\n",
        "* 11214912\n",
        "* 15896912\n",
        "* 20072512"
      ],
      "metadata": {
        "id": "U6jKpJ6o3T7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNModel, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels=3,\n",
        "            out_channels=32,\n",
        "            kernel_size=3\n",
        "        )\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # After conv+pool:\n",
        "        # Input: (3, 200, 200)\n",
        "        # Conv -> (32, 198, 198)\n",
        "        # Pool -> (32, 99, 99)\n",
        "        self.flatten_size = 32 * 99 * 99\n",
        "\n",
        "        self.fc1 = nn.Linear(self.flatten_size, 64)\n",
        "        self.fc_out = nn.Linear(64, 1)   # single output for binary classification\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "\n",
        "        # Output: raw score (logit)\n",
        "        return self.fc_out(x)\n",
        "\n",
        "# Example: recommended loss\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Optimizer\n",
        "model = CNNModel()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.002, momentum=0.8)\n"
      ],
      "metadata": {
        "id": "SxYYpH5x-Udd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Option 1: Using torchsummary (install with: pip install torchsummary)\n",
        "from torchsummary import summary\n",
        "summary(model, input_size=(3, 200, 200))"
      ],
      "metadata": {
        "id": "suTf7j8k550O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e991eb9-8b82-49d8-b16f-e86fa3051391"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 32, 198, 198]             896\n",
            "         MaxPool2d-2           [-1, 32, 99, 99]               0\n",
            "            Linear-3                   [-1, 64]      20,072,512\n",
            "            Linear-4                    [-1, 1]              65\n",
            "================================================================\n",
            "Total params: 20,073,473\n",
            "Trainable params: 20,073,473\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.46\n",
            "Forward/backward pass size (MB): 11.96\n",
            "Params size (MB): 76.57\n",
            "Estimated Total Size (MB): 89.00\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The total number of parameters of the model is **896** !!!"
      ],
      "metadata": {
        "id": "HYa3BqQp-mtf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generators and Training\n",
        "\n",
        "For the next two questions, use the following transformation for both train and test sets:\n",
        "```\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((200, 200)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    ) # ImageNet normalization\n",
        "])\n",
        "```\n",
        "* We don't need to do any additional pre-processing for the images.\n",
        "* Use batch_size=20\n",
        "* Use shuffle=True for both training, but False for test."
      ],
      "metadata": {
        "id": "ToPwu37U3TzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((200, 200)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    ) # ImageNet normalization\n",
        "])"
      ],
      "metadata": {
        "id": "hC9RuxeO_7MV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now fit the model.\n",
        "\n",
        "You can use this code:\n",
        "```\n",
        "num_epochs = 10\n",
        "history = {'acc': [], 'loss': [], 'val_acc': [], 'val_loss': []}\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        labels = labels.float().unsqueeze(1) # Ensure labels are float and have shape (batch_size, 1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        # For binary classification with BCEWithLogitsLoss, apply sigmoid to outputs before thresholding for accuracy\n",
        "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "    epoch_acc = correct_train / total_train\n",
        "    history['loss'].append(epoch_loss)\n",
        "    history['acc'].append(epoch_acc)\n",
        "\n",
        "    model.eval()\n",
        "    val_running_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in validation_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            labels = labels.float().unsqueeze(1)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_running_loss += loss.item() * images.size(0)\n",
        "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    val_epoch_loss = val_running_loss / len(validation_dataset)\n",
        "    val_epoch_acc = correct_val / total_val\n",
        "    history['val_loss'].append(val_epoch_loss)\n",
        "    history['val_acc'].append(val_epoch_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
        "          f\"Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}, \"\n",
        "          f\"Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}\"))\n",
        "```"
      ],
      "metadata": {
        "id": "F07PImeV_7md"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dY2i_2_gXHK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# -------------------- Device --------------------\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# -------------------- Data Transforms --------------------\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((200, 200)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((200, 200)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# -------------------- Datasets --------------------\n",
        "# Make sure your folders are structured like this:\n",
        "# data/train/straight/, data/train/curve/\n",
        "# data/test/straight/, data/test/curve/\n",
        "train_dataset = datasets.ImageFolder(root='data/train', transform=train_transforms)\n",
        "test_dataset = datasets.ImageFolder(root='data/test', transform=test_transforms)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# -------------------- Model --------------------\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128*25*25, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, 1)  # Binary classification\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "model = SimpleCNN().to(device)\n",
        "\n",
        "# -------------------- Loss & Optimizer --------------------\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# -------------------- Training Loop --------------------\n",
        "num_epochs = 10\n",
        "history = {'acc': [], 'loss': [], 'val_acc': [], 'val_loss': []}\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        labels = labels.float().unsqueeze(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "    epoch_acc = correct_train / total_train\n",
        "    history['loss'].append(epoch_loss)\n",
        "    history['acc'].append(epoch_acc)\n",
        "\n",
        "    # -------------------- Validation (using test set) --------------------\n",
        "    model.eval()\n",
        "    val_running_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            labels = labels.float().unsqueeze(1)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_running_loss += loss.item() * images.size(0)\n",
        "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    val_epoch_loss = val_running_loss / len(test_dataset)\n",
        "    val_epoch_acc = correct_val / total_val\n",
        "    history['val_loss'].append(val_epoch_loss)\n",
        "    history['val_acc'].append(val_epoch_acc)\n",
        "\n",
        "    # -------------------- Print --------------------\n",
        "    print(\n",
        "        f\"Epoch {epoch+1}/{num_epochs}, \"\n",
        "        f\"Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}, \"\n",
        "        f\"Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}\"\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzUKE8Yr_78F",
        "outputId": "86618a18-4737-4ee5-b7a7-e57227b9cb96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Epoch 1/10, Loss: 0.7869, Acc: 0.5787, Val Loss: 0.7391, Val Acc: 0.5622\n",
            "Epoch 2/10, Loss: 0.6343, Acc: 0.6587, Val Loss: 0.6152, Val Acc: 0.6368\n",
            "Epoch 3/10, Loss: 0.5602, Acc: 0.6975, Val Loss: 0.6154, Val Acc: 0.6468\n",
            "Epoch 4/10, Loss: 0.5149, Acc: 0.7512, Val Loss: 0.5642, Val Acc: 0.6667\n",
            "Epoch 5/10, Loss: 0.4643, Acc: 0.7825, Val Loss: 0.5220, Val Acc: 0.7413\n",
            "Epoch 6/10, Loss: 0.4132, Acc: 0.8113, Val Loss: 0.5758, Val Acc: 0.6965\n",
            "Epoch 7/10, Loss: 0.3508, Acc: 0.8538, Val Loss: 0.6578, Val Acc: 0.7114\n",
            "Epoch 8/10, Loss: 0.3363, Acc: 0.8538, Val Loss: 0.5475, Val Acc: 0.7662\n",
            "Epoch 9/10, Loss: 0.1974, Acc: 0.9263, Val Loss: 0.6078, Val Acc: 0.7910\n",
            "Epoch 10/10, Loss: 0.1171, Acc: 0.9637, Val Loss: 0.6666, Val Acc: 0.7910\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3**\n",
        "\n",
        "What is the median of training accuracy for all the epochs for this model?\n",
        "\n",
        "* 0.05\n",
        "* 0.12\n",
        "* 0.40\n",
        "* 0.84"
      ],
      "metadata": {
        "id": "1nAsWvAD3Tkq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# accuracy stored in variable history\n",
        "history['acc']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIXTkLSatu0N",
        "outputId": "8e53e8de-6334-4ed3-eb81-6a6c62d3fb60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.57875,\n",
              " 0.65875,\n",
              " 0.6975,\n",
              " 0.75125,\n",
              " 0.7825,\n",
              " 0.81125,\n",
              " 0.85375,\n",
              " 0.85375,\n",
              " 0.92625,\n",
              " 0.96375]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "median_train_acc = np.median(history['acc'])\n",
        "print(\"Median Training Accuracy:\",round(median_train_acc,2))"
      ],
      "metadata": {
        "id": "R1jh4AXB561W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "389ae354-5b0b-4f15-b8eb-ccdaaea2795e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Median Training Accuracy: 0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4**\n",
        "\n",
        "What is the standard deviation of training loss for all the epochs for this model?\n",
        "\n",
        "* 0.007\n",
        "* 0.078\n",
        "* 0.171\n",
        "* 1.710\n"
      ],
      "metadata": {
        "id": "074oM7I83TVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "std_train_loss = np.std(history['loss'])\n",
        "std_train_loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7W8sLMTxuvW9",
        "outputId": "ca534a71-a112-4531-db24-97db6c73cc04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.19005064521943707)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Standard deviation of training loss:\",round(std_train_loss,2))"
      ],
      "metadata": {
        "id": "eObg4trc57p2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e719856a-3c4a-4005-8cf7-bc5737481763"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standard deviation of training loss: 0.19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Augmentation**\n",
        "For the next two questions, we'll generate more data using data augmentations.\n",
        "\n",
        "Add the following augmentations to your training data generator:\n",
        "```\n",
        "transforms.RandomRotation(50),\n",
        "transforms.RandomResizedCrop(200, scale=(0.9, 1.0), ratio=(0.9, 1.1)),\n",
        "transforms.RandomHorizontalFlip(),\n",
        "```"
      ],
      "metadata": {
        "id": "YZPYPyr33wnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# -------------------- Device --------------------\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# -------------------- Data Transforms --------------------\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((200, 200)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "    transforms.RandomRotation(50),\n",
        "    transforms.RandomResizedCrop(200, scale=(0.9, 1.0), ratio=(0.9, 1.1)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((200, 200)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# -------------------- Datasets --------------------\n",
        "# Make sure your folders are structured like this:\n",
        "# data/train/straight/, data/train/curve/\n",
        "# data/test/straight/, data/test/curve/\n",
        "train_dataset = datasets.ImageFolder(root='data/train', transform=train_transforms)\n",
        "test_dataset = datasets.ImageFolder(root='data/test', transform=test_transforms)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# -------------------- Model --------------------\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128*25*25, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, 1)  # Binary classification\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "model = SimpleCNN().to(device)\n",
        "\n",
        "# -------------------- Loss & Optimizer --------------------\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# -------------------- Training Loop --------------------\n",
        "num_epochs = 20\n",
        "history = {'acc': [], 'loss': [], 'val_acc': [], 'val_loss': []}\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        labels = labels.float().unsqueeze(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "    epoch_acc = correct_train / total_train\n",
        "    history['loss'].append(epoch_loss)\n",
        "    history['acc'].append(epoch_acc)\n",
        "\n",
        "    # -------------------- Validation (using test set) --------------------\n",
        "    model.eval()\n",
        "    val_running_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            labels = labels.float().unsqueeze(1)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_running_loss += loss.item() * images.size(0)\n",
        "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    val_epoch_loss = val_running_loss / len(test_dataset)\n",
        "    val_epoch_acc = correct_val / total_val\n",
        "    history['val_loss'].append(val_epoch_loss)\n",
        "    history['val_acc'].append(val_epoch_acc)\n",
        "\n",
        "    # -------------------- Print --------------------\n",
        "    print(\n",
        "        f\"Epoch {epoch+1}/{num_epochs}, \"\n",
        "        f\"Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}, \"\n",
        "        f\"Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}\"\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4KYf0SVLrD1",
        "outputId": "55ff00af-ef9b-4697-e0b0-14a3a3422f64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Epoch 1/20, Loss: 0.8436, Acc: 0.5563, Val Loss: 0.6427, Val Acc: 0.6418\n",
            "Epoch 2/20, Loss: 0.6397, Acc: 0.6388, Val Loss: 0.6168, Val Acc: 0.6517\n",
            "Epoch 3/20, Loss: 0.6396, Acc: 0.6288, Val Loss: 0.6068, Val Acc: 0.6716\n",
            "Epoch 4/20, Loss: 0.6125, Acc: 0.6625, Val Loss: 0.6191, Val Acc: 0.6418\n",
            "Epoch 5/20, Loss: 0.6082, Acc: 0.6687, Val Loss: 0.5772, Val Acc: 0.7065\n",
            "Epoch 6/20, Loss: 0.5623, Acc: 0.7250, Val Loss: 0.5363, Val Acc: 0.7413\n",
            "Epoch 7/20, Loss: 0.5557, Acc: 0.7200, Val Loss: 0.5256, Val Acc: 0.7065\n",
            "Epoch 8/20, Loss: 0.5242, Acc: 0.7400, Val Loss: 0.5310, Val Acc: 0.7214\n",
            "Epoch 9/20, Loss: 0.4927, Acc: 0.7700, Val Loss: 0.4877, Val Acc: 0.7662\n",
            "Epoch 10/20, Loss: 0.4761, Acc: 0.7825, Val Loss: 0.4982, Val Acc: 0.7512\n",
            "Epoch 11/20, Loss: 0.4443, Acc: 0.7950, Val Loss: 0.4401, Val Acc: 0.7861\n",
            "Epoch 12/20, Loss: 0.4429, Acc: 0.7975, Val Loss: 0.4294, Val Acc: 0.7910\n",
            "Epoch 13/20, Loss: 0.4405, Acc: 0.8087, Val Loss: 0.4076, Val Acc: 0.8060\n",
            "Epoch 14/20, Loss: 0.4219, Acc: 0.8113, Val Loss: 0.3756, Val Acc: 0.8209\n",
            "Epoch 15/20, Loss: 0.4015, Acc: 0.8337, Val Loss: 0.3740, Val Acc: 0.8458\n",
            "Epoch 16/20, Loss: 0.3596, Acc: 0.8500, Val Loss: 0.3550, Val Acc: 0.8408\n",
            "Epoch 17/20, Loss: 0.3465, Acc: 0.8413, Val Loss: 0.3819, Val Acc: 0.8358\n",
            "Epoch 18/20, Loss: 0.3260, Acc: 0.8650, Val Loss: 0.3878, Val Acc: 0.8159\n",
            "Epoch 19/20, Loss: 0.3085, Acc: 0.8712, Val Loss: 0.3215, Val Acc: 0.8657\n",
            "Epoch 20/20, Loss: 0.2882, Acc: 0.8888, Val Loss: 0.3495, Val Acc: 0.8408\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5**\n",
        "\n",
        "Let's train our model for 10 more epochs using the same code as previously.\n",
        "\n",
        "`Note: make sure you don't re-create the model. we want to continue training the model we already started training.`\n",
        "\n",
        "What is the mean of test loss for all the epochs for the model trained with augmentations?\n",
        "\n",
        "* 0.008\n",
        "* 0.08\n",
        "* 0.88\n",
        "* 8.88"
      ],
      "metadata": {
        "id": "xIgv7J21357w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_train_acc = np.mean(history['acc'])\n",
        "print(\"Mean Training Accuracy:\", round(mean_train_acc, 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjmUnS-y3dIH",
        "outputId": "b5021ee1-a920-4846-fb98-7e45eec4a9b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Training Accuracy: 0.76\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6**\n",
        "\n",
        "What's the average of test accuracy for the last 5 epochs (from 6 to 10) for the model trained with augmentations?\n",
        "\n",
        "* 0.08\n",
        "* 0.28\n",
        "* 0.68\n",
        "* 0.98"
      ],
      "metadata": {
        "id": "w15dWFWm3-RI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from the last 5 epochs (from 6 to 10)\n",
        "last_6epochs = history['acc'][5:10]\n",
        "last_6epochs"
      ],
      "metadata": {
        "id": "edeAsoAl59xF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "312557f0-fd40-437b-8962-ceb0569e5ae4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.725, 0.72, 0.74, 0.77, 0.7825]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean_train_acc_last6 = np.mean(last_6epochs)\n",
        "print(\"Mean Training Accuracy:\", round(mean_train_acc_last6, 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stD9SuUD4jYH",
        "outputId": "1278e5a8-2d6f-4673-cbd3-fe3fa0933d95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Training Accuracy: 0.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Submit the results\n",
        "Submit your results here: https://courses.datatalks.club/ml-zoomcamp-2025/homework/hw08homework/hw08\n",
        "If your answer doesn't match options exactly, select the closest one. If the answer is exactly in between two options, select the higher value."
      ],
      "metadata": {
        "id": "z97UwDRgjIsX"
      }
    }
  ]
}